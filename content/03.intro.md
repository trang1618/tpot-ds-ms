## Introduction

For many bioinformatics problems of classifying individuals into clinical categories from high-dimensional biological data, performance of a machine learning (ML) model depends greatly on the problem it is applied to [@doi:10.1186/s13040-017-0154-4;@pmid:29218881].
In addition, choosing a classifier is merely one step of the arduous process that leads to predictions.
To detect patterns among features (*e.g.*, clinical variables) and their associations with the outcome (*e.g.*, clinical diagnosis), a data scientist typically has to design and test different complex machine learning (ML) frameworks that consist of data exploration, feature engineering, model selection and prediction.
Automated machine learning (AutoML) systems were developed to automate this challenging and time-consuming process.
These intelligent systems increase the accessibility and scalability of various machine learning applications by efficiently solving an optimization problem to discover pipelines that yield satisfactory outcomes, such as prediction accuracy.
Consequently, AutoML allows data scientists to focus their effort in applying their expertise in other important research components such as developing meaningful hypotheses or communicating the results.

Various approaches have been employed to build AutoML systems for diverse applications.
Auto-sklearn [@url:https://papers.nips.cc/paper/5872-efficient-and-robust-automated-machine-learning] and Auto-WEKA [@arxiv:1208.3719; @url:http://www.jmlr.org/papers/v18/16-261.html] use Bayesian optimization for model selection and hyperparameter optimization.
Recipe [@doi:10.1007/978-3-319-55696-3_16] optimizes the ML pipeline through grammar-based genetic programming and Autostacker [@arxiv:1803.00684] automates stacked ensembling.
Both methods automate hyperparameter tuning and model selection using evolutionary algorithm.
DEvol (https://github.com/joeddav/devol) designs deep neural network specifically via genetic programming.
H2O.ai (http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html) automates data preprocessing, hyperparameter tuning, random grid search and stacked ensembles in a distributed ML platform in multiple languages.
Finally, Xcessiv (https://github.com/reiinakano/xcessiv) provides web-based application for quick, scalable, and automated hyper-parameter tuning and stacked ensembling in Python.

Tree-based Pipeline Optimization Tool (TPOT) is a genetic programming-based AutoML system that uses genetic programming (GP) [@raw:GPbook] to optimize a series of feature selectors, preprocessors and ML models with the objective of maximizing classification accuracy.
While most AutoML systems primarily focus on model selection and hyperparameter optimization, TPOT also pays attention to feature selection and feature engineering by evaluating the complete pipelines based on their cross-validated score such as mean squared error or balanced accuracy.
Given no a priori knowledge about the problem, TPOT has been shown to frequently outperform standard machine learning analyses [@doi:10.1145/2908812.2908918; @arxiv:1607.08878].
Effort has been made to specialize TPOT for human genetics research, resulting in a useful extended version of TPOT, TPOT-MDR, that features Multifactor Dimensionality Reduction and an Expert Knowledge Filter [@doi:10.1145/3071178.3071212].
However, at the current stage, TPOT still requires great computational expense to analyze large datasets such as in genome-wide association studies (GWAS) or gene expression analyses. 
Consequently, the application of TPOT on real-world datasets has been limited to small sets of features [@arxiv:1803.04487].

In this work, we introduce two new features implemented in TPOT that helps increase the systemâ€™s scalability.
First, the Dataset Selector (DS) allows the users to pass specific subsets of the features, reducing the computational expense of TPOT at the beginning of each pipeline to only evaluate on a smaller subset of data rather than the entire dataset.
Consequently, DS increases TPOT's efficiency in application on large data sets by slicing the data into smaller sets of features (*e.g.* genes) and allowing a genetic algorithm to select the best subset in the final pipeline.
Second, Template enables the option for strongly typed GP, a method to enforce type constraints in genetic programming.
By letting users specify a desired structure of the resulting machine learning pipeline, Template helps reduce TPOT computation time and potentially provide more interpretable results.
