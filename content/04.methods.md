## Methods
We begin with descriptions of the two novel additions to TPOT, Feature Set Selector and Template.
Then, we provide detail of a real-world RNA-Seq expression dataset and describe a simulation approach to generate data comparable to the expression data.
Finally, we discuss other methods and performance metrics for comparison.
Detailed simulation and analysis code needed to reproduce the results has been made available on the GitHub repository [https://github.com/lelaboratoire/tpot-fss](https://github.com/lelaboratoire/tpot-fss).

### Tree-based Pipeline Optimization Tool
Tree-based Pipeline Optimization Tool (TPOT) automates the laborious process of designing a ML pipeline by representing pipelines as binary expression trees with ML operators as primitives.
Pipeline elements include algorithms from the extensive library of scikit-learn [@url:https://hal.archives-ouvertes.fr/hal-00650905/] as well as other efficient implementations such as extreme gradient boosting.
Applying GP with the NSGA-II Pareto optimization [@doi:10.1109/4235.996017], TPOT optimizes the accuracy achieved by the pipeline while accounting for its complexity.
Specifically, to automatically generate and optimize these machine learning pipelines, TPOT utilizes the Python package DEAP [@url:http://www.jmlr.org/papers/v13/fortin12a.html] to implement the GP algorithm.
Implementation details can be found at TPOT's active Github repository [https://github.com/EpistasisLab/tpot](https://github.com/EpistasisLab/tpot).

### Feature Set Selector 
TPOT's current operators include sets of feature pre-processors, feature transformers, feature selection techniques, and supervised classifiers and regressions.
In this study, we introduce a new operator called Feature Set Selector (FSS) that enables biologically guided group-level feature selection.
From predefined subsets of features, the FSS operator allows TPOT to select the best subset that maximizes average accuracy in *k*-fold cross validation (5-fold by default).
Specifically, taking place at the very first stage of the pipeline, FSS passes only a specific subset of the features onwards, effectively slicing the large original dataset into smaller ones.
Hence, with FSS, users can specify subsets of features of interest to reduce the feature space’s dimension at pipeline initialization.

For example, in a gene expression analysis of major depressive disorder, a neuroscientist can specify collections of genes in pathways of interest and identify the important collection that helps predict the depression severity.
Similarly, in a genome-wide association study of breast cancer, an analyst may assign variants in the data to different subsets of potentially related variants and detect the subset associated with the breast cancer diagnosis.
In general, the FSS operator takes advantage of previous compartmentalization of the feature space to smaller subsets based on *a priori* expert knowledge about the biomedical dataset.
From here, TPOT learns and selects the most relevant group of features for outcome prediction.
Compared to TPOT's existing Selector operators, FSS selects features at the group level instead of individual level.

### Template
Parallel with the establishment of the Feature Set Selector operator, we now offer TPOT users the option to define a Template that provides a way to specify a desired structure for the resulting machine learning pipeline, which will reduce TPOT computation time and potentially provide more interpretable results.

Current implementation of Template supports linear pipelines, or path graphs, which are trees with two nodes (operators) of vertex degree 1, and the other $n-2$ nodes of vertex degree 2.
Further, Template takes advantage of the strongly typed genetic programming framework that enforces data-type constraints [@doi:10.1162/evco.1995.3.2.199] and imposes type-based restrictions on which element (*i.e.*, operator) type can be chosen at each node.
In strongly typed genetic programming, while the fitness function and parameters remain the same, the initialization procedure and genetic operators (*e.g.*, mutation, crossover) must respect the enhanced legality constraints [@doi:10.1162/evco.1995.3.2.199].
With a Template defined, each node in the tree pipeline is assigned one of the five major operator types: feature set selector, feature selector, feature transformer, classifier or regressor. 
Moreover, besides the major operator types, each node can also be assigned more specifically as a method of an operator, such as decision trees for classifier. 
An example Template is Feature set selector &rarr; Feature transform &rarr; Decision trees.

### Datasets
We apply TPOT with the new FSS operator on both simulated datasets and a real world RNA-Seq gene expression dataset. 
With both real-world and simulated data, we hope to acquire a comprehensive view of the strengths and limitations of TPOT in the next generation sequencing domain.

#### Simulation methods
The simulated datasets were generated using the `R` package `privateEC`, which was designed to simulate realistic effects to be expected in gene expression or resting-state fMRI data. 
In the current study, to be consistent with the real expression dataset (described below), we simulate interaction effect data with *m* = 200 individuals (100 cases and 100 controls) and *p* = 5,000 real-valued features with 4% functional (true positive association with outcome) for each training and testing set. 
Full details of the simulation approach can be found in Refs. [@doi:10.1038/s41398-018-0234-3; @doi:10.1186/s13040-015-0040-x]. Briefly, the privateEC simulation induces a differential co-expression network of random normal expression levels and permutes the values of targeted features within the cases to generate interactions. 
Further, by imposing a large number of background features (no association with outcome), we seek to assess TPOT-FSS’s performance in accommodating large numbers of non-predictive features.

To closely resemble the module size distribution in the RNA-Seq data, we first fit a $\Gamma$ distribution to the observed module sizes then sample from this distribution values for the simulated subset size, before the total number of features reaches 4,800 (number of background features). Then, the background features were randomly placed in each subset corresponding to its size. Also, for each subset $S_i, i = 1, \dots, n$, a functional feature $s_j$ belongs to the subset with the probability
$$P(s_j \in S_i) \sim 1.618^{-i}$$ {#eq:p_subset}
where 1.618 is an approximation of the golden ratio and yields a reasonable distribution of the functional features: they are more likely to be included in the earlier subsets (subset 1 and 2) than the later ones. 

#### Real-world RNA-Seq expression data
We employed TPOT-FSS on an RNA-Seq expression dataset of 78 individuals with major depressive disorder (MDD) and 79 healthy controls (HC) from Ref. [@doi:10.1038/s41398-018-0234-3]. 
Gene expression levels were quantified from reads of 19,968 annotated protein-coding genes and underwent a series of preprocessing steps including low read-count and outlier removal, technical and batch effect adjustment, and coefficient of variation filtering. 
Consequently, whole blood RNA-Seq measurements of 5,912 genes were obtained and are now used in the current study to test for association with MDD status. 
We use the 23 subsets of interconnected genes called depression gene modules (DGMs) identified from the RNA-Seq gene network module analysis [@doi:10.1038/s41398-018-0234-3] as input for the FSS operator.
We remark that these modules were constructed by an unsupervised machine learning method with dynamic tree cutting from a co-expression network.
As a result, this prior knowledge of the gene structure does not depend on the diagnostic phenotype and thus yields no bias in the downstream analysis of TPOT-FSS.

### Performance assessment
For each simulated and real-world dataset, after randomly splitting the entire data in two balanced smaller sets (75% training and 25% holdout), we trained TPOT-FSS with the Template `FeatureSetSelector-Transformer-Classifier` on training data to predict class (*e.g.*, diagnostic phenotype in real-world data) in the holdout set.
We assess the performance of TPOT-FSS by quantifying its ability to correctly select the most important subset (containing most functional features) in 100 replicates of TPOT runs on simulated data with known underlying truth.
To prevent potential overfitting, we select the pipeline that is closest to the 90th percentile of the cross-validation accuracy to be optimal.
This rationale is motivated by a similar procedure for optimizing the penalty coefficient in regularized regression where the most parsimonious model within one standard error of the minimum cross-validation error is picked [@isbn:9780387848570].
We compare the holdout (out-of-sample) accuracy of TPOT-FSS's optimal pipeline on the holdout set with that of standard TPOT (with `Transformer-Classifier` Template, no FSS operator) and eXtreme Gradient Boosting [@doi:10.1145/2939672.2939785], or XGBoost, which is a fast and an efficient implementation of the gradient tree boosting method that has shown much utility in many winning Kaggle solutions (https://www.kaggle.com/) and been successfully incorporated in several neural network architectures [@doi:10.3390/en10081168;@doi:10.1007/978-3-319-64185-0_28].
In the family of gradient boosted decision trees, XGBoost accounts for complex non-linear interaction structure among features and leverages gradient descents and boosting (sequential ensemble of weak classifiers) to effectively produce a strong prediction model.
To obtain the optimal performance for this baseline model, we tune XGBoost hyperparameters using TPOT Template with only one classifier `XGBClassifier`, which is imported from the `xgboost` python package. 
Because of stochasticity in the optimal pipeline from TPOT-FSS, standard TPOT and the tuned XGBoost model, we fit these models on the training data 100 times and compare 100 holdout accuracy values from each method.
We choose accuracy to be the metric for comparison because phenotype is balanced in both simulated data and real-world data.

### Manuscript drafting
This manuscript is collaboratively written using Manubot [@url:https://greenelab.github.io/meta-review/], a software that supports open paper writing via GitHub using the Markdown language.
Manubot uses continuous integration to monitor changes and automatically update the manuscript.
Consequently, the latest version of this manuscript is always available at [https://trang1618.github.io/tpot-fss-ms/](https://trang1618.github.io/tpot-fss-ms/).