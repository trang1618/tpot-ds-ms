## Introduction

For many bioinformatics problems of classifying individuals into clinical categories from high-dimensional biological data, choosing a classifier is merely one step of the arduous process that leads to predictions.
To detect patterns among features (*e.g.*, clinical variables) and their associations with the outcome (*e.g.*, clinical diagnosis), a data scientist typically has to design and test different complex machine learning frameworks that consist of data exploration, feature engineering, model selection and prediction.
Automated machine learning (AutoML) systems were developed to automate this challenging and time-consuming process.
These intelligent systems increase the accessibility and scalability of various machine learning applications by efficiently solving an optimization problem to discover pipelines that yield satisfactory outcomes, such as prediction accuracy.
Consequently, AutoML allows data scientists to focus their effort in applying their expertise in other important research components such as developing meaningful hypotheses or communicating the results.

To our knowledge, there are several AutoML systems. Both auto-sklearn [@url:https://papers.nips.cc/paper/5872-efficient-and-robust-automated-machine-learning] and Auto-WEKA [@arxiv:1208.3719, @url:http://www.jmlr.org/papers/v18/16-261.html] uses Bayesian optimization for machine learning model/pipeline selection and hyperparameter optimization.
Both Recipe [@doi:10.1007/978-3-319-55696-3_16] and Autostacker [@arxiv:1803.00684] automate hyperparameter tuning and model selection using evaluation algorithm.
Recipe is pipeline optimization through grammar-based genetic programming using grammars to define pipeline structure, while autostacker automates stacked ensembling.
And DEvol [@url:https://github.com/joeddav/devol] automates deep neural network design specifically via genetic programming.
H2O.ai [@url:http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html] automates data preprocessing, hyperparameter tuning, random grid search and stacked ensembles in a distributed ML platform in multiple languages.
Finally, Xcessiv [@url:https://github.com/reiinakano/xcessiv] provides web-based application for quick, scalable, and automated hyper-parameter tuning and stacked ensembling in Python.

Tree-based Pipeline Optimization Tool (TPOT) is AutoML system using strongly typed genetic programming and it automates the laborious process of designing a machine learning pipeline to solve a supervised learning problem.
At its core, TPOT uses genetic programming (GP) [@raw:GPbook] to optimize a series of feature selectors, preprocessors and machine learning models with the objective of maximizing classification accuracy.
While most AutoML systems primarily focus on model selection and hyperparameter optimization, TPOT also pays attention to feature selection and feature engineering in building a complete pipeline. Applying GP with the NSGA-II Pareto optimization [@doi:10.1109/4235.996017], TPOT optimizes the accuracy achieved by the pipeline while accounting for its complexity.
Specifically, to automatically generate and optimize these machine learning pipelines, TPOT utilizes the Python package DEAP [@url:http://www.jmlr.org/papers/v13/fortin12a.html] to implement the GP algorithm.		

Given no a priori knowledge about the problem, TPOT has been showed to frequently outperform standard machine learning analyses [@doi:10.1145/2908812.2908918; @arxiv:1607.08878].
Effort has been made to specialize TPOT for human genetics research, which results in a useful extended version of TPOT, TPOT-MDR, that features Multifactor Dimensionality Reduction and an Expert Knowledge Filter [@doi:10.1145/3071178.3071212].
However, at the current stage, TPOT still requires great computational expense to analyze large datasets such as in genome-wide association studies or gene expression analyses. Consequently, application of TPOT on real-world datasets has been limited to small sets of features [@arxiv:1803.04487].

In this work, we introduce two new features implemented in TPOT that helps increase the systemâ€™s scalability.
First, the Dataset Selector (DS) allows the users to pass specific subsets of the features, reducing the computational expense of TPOT at the beginning of each pipeline to only evaluate on a smaller subset of data rather than the entire dataset.
Consequently, DS makes TPOT applicable on large data sets by slicing the data into smaller sets of features (*e.g.* genes) and allowing genetic algorithm to select the best subset in the final pipeline.
Second, Template enables the option for strongly typed GP, a method to enforce type constraints in genetic programming.
By letting users specify a desired structure of the resulting machine learning pipeline, Template helps reduce TPOT computation time and potentially provide more interpretable results.
