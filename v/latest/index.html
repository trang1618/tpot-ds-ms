<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Trang T. Le" />
  <meta name="author" content="Weixuan Fu" />
  <meta name="author" content="Jason H. Moore" />
  <meta name="dcterms.date" content="2018-11-15" />
  <meta name="keywords" content="tpot, automl, machine learning" />
  <title>Scaling tree-based automated machine learning to biomedical big data with a dataset selector</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="github-pandoc.css" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <!-- Insert Analytics Script Below -->
  <script>
  </script>
  <!-- End Analytics Script -->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Scaling tree-based automated machine learning to biomedical big data with a dataset selector</h1>
</header>
<p><small><em> This manuscript (<a href="https://trang1618.github.io/tpot-ds-ms/v/32d59513031e267ae88841a22391fa4c1b09e3d4/">permalink</a>) was automatically generated from <a href="https://github.com/trang1618/tpot-ds-ms/tree/32d59513031e267ae88841a22391fa4c1b09e3d4">trang1618/tpot-ds-ms@32d5951</a> on November 15, 2018. </em></small></p>
<h2 id="authors">Authors</h2>
<ul>
<li><p><strong>Trang T. Le</strong><sup>☯</sup><br> <img src="images/orcid.svg" alt="ORCID icon" width="13" height="13" /> <a href="https://orcid.org/0000-0003-3737-6565">0000-0003-3737-6565</a> · <img src="images/github.svg" alt="GitHub icon" width="13" height="13" /> <a href="https://github.com/trang1618">trang1618</a> · <img src="images/twitter.svg" alt="Twitter icon" width="13" height="13" /> <a href="https://twitter.com/trang1618">trang1618</a><br> <small> Department of Biostatistics, Epidemiology and Informatics, Institute for Biomedical Informatics, University of Pennsylvania, Philadelphia, PA 19104 </small></p></li>
<li><p><strong>Weixuan Fu</strong><sup>☯</sup><br> <img src="images/orcid.svg" alt="ORCID icon" width="13" height="13" /> <a href="https://orcid.org/0000-0002-6434-5468">0000-0002-6434-5468</a> · <img src="images/github.svg" alt="GitHub icon" width="13" height="13" /> <a href="https://github.com/weixuanfu">weixuanfu</a> · <img src="images/twitter.svg" alt="Twitter icon" width="13" height="13" /> <a href="https://twitter.com/weixuanfu">weixuanfu</a><br> <small> Department of Biostatistics, Epidemiology and Informatics, Institute for Biomedical Informatics, University of Pennsylvania, Philadelphia, PA 19104 </small></p></li>
<li><p><strong>Jason H. Moore</strong><sup>†</sup><br> <img src="images/orcid.svg" alt="ORCID icon" width="13" height="13" /> <a href="https://orcid.org/0000-0002-5015-1099">0000-0002-5015-1099</a> · <img src="images/github.svg" alt="GitHub icon" width="13" height="13" /> <a href="https://github.com/EpistasisLab">EpistasisLab</a> · <img src="images/twitter.svg" alt="Twitter icon" width="13" height="13" /> <a href="https://twitter.com/moorejh">moorejh</a><br> <small> Department of Biostatistics, Epidemiology and Informatics, Institute for Biomedical Informatics, University of Pennsylvania, Philadelphia, PA 19104 · Funded by National Institute of Health Grant Nos. LM010098, LM012601 </small></p></li>
</ul>
<p><sup>☯</sup> — These authors contributed equally to this work.</p>
<p><sup>†</sup> — Direct correspondence to jhmoore@upenn.edu.</p>
<h2 id="abstract" class="page_break_before">Abstract</h2>
<p>Automated machine learning (AutoML) systems were considered as good data science assistant for automating time-consuming processes, like feature engineering, model selection and parameter optimization. For this purpose, TPOT was developed for optimizing machine learning pipelines using strongly typed genetic programming and it keeps helping data scientists to search optimized analysis pipeline on their specific datasets. But TPOT may suffer computational resource limits when working on big data, such as whole-genome expression data. We introduce two new features implemented in TPOT that helps increase the system’s scalability: dataset selector (DS) and Template. Dataset selector provides the option to specify subsets of the features, reducing the computational expense of TPOT at the beginning of each pipeline to only evaluate on a smaller subset of data rather than the entire dataset. Consequently, DS makes TPOT applicable on large data sets by slicing the data into smaller sets of features and allowing genetic algorithm to select the best subset in the final pipeline. Template enforces type constraints with strongly typed genetic programming. We show that DS and Template help reduce TPOT computation time and potentially provide more interpretable results. Independent of a previous study that identified significant association with depressions severity of the enrichment scores of two modules, we find with TPOT-DS that one of the modules is largely predictive of the clinical diagnosis of each individual.</p>
<h2 id="introduction">Introduction</h2>
<p>For many bioinformatics problems of classifying individuals into clinical categories from high-dimensional biological data, choosing a classifier is merely one step of the arduous process that leads to predictions. To detect patterns among features (<em>e.g.</em>, clinical variables) and their associations with the outcome (<em>e.g.</em>, clinical diagnosis), a data scientist typically has to design and test different complex machine learning frameworks that consist of data exploration, feature engineering, model selection and prediction. Automated machine learning (AutoML) systems were developed to automate this challenging and time-consuming process. These intelligent systems increase the accessibility and scalability of various machine learning applications by efficiently solving an optimization problem to discover pipelines that yield satisfactory outcomes, such as prediction accuracy. Consequently, AutoML allows data scientists to focus their effort in applying their expertise in other important research components such as developing meaningful hypotheses or communicating the results.</p>
<p>To our knowledge, there are several AutoML systems. Both auto-sklearn <span class="citation" data-cites="8JQDv397">[<a href="#ref-8JQDv397">1</a>]</span> and Auto-WEKA <span class="citation" data-cites="S6aZVb3n">[<a href="#ref-S6aZVb3n">2</a>]</span> uses Bayesian optimization for machine learning model/pipeline selection and hyperparameter optimization. Both Recipe <span class="citation" data-cites="6ChydIkb">[<a href="#ref-6ChydIkb">4</a>]</span> and Autostacker <span class="citation" data-cites="RiocGZOq">[<a href="#ref-RiocGZOq">5</a>]</span> automate hyperparameter tuning and model selection using evaluation algorithm. Recipe is pipeline optimization through grammar-based genetic programming using grammars to define pipeline structure, while autostacker automates stacked ensembling. And DEvol <span class="citation" data-cites="1CQSBxFFr">[<a href="#ref-1CQSBxFFr">6</a>]</span> automates deep neural network design specifically via genetic programming. H2O.ai <span class="citation" data-cites="sOdEzGT3">[<a href="#ref-sOdEzGT3">7</a>]</span> automates data preprocessing, hyperparameter tuning, random grid search and stacked ensembles in a distributed ML platform in multiple languages. Finally, Xcessiv <span class="citation" data-cites="HA8l3lpi">[<a href="#ref-HA8l3lpi">8</a>]</span> provides web-based application for quick, scalable, and automated hyper-parameter tuning and stacked ensembling in Python.</p>
<p>Tree-based Pipeline Optimization Tool (TPOT) is AutoML system using strongly typed genetic programming and it automates the laborious process of designing a machine learning pipeline to solve a supervised learning problem. At its core, TPOT uses genetic programming (GP) <span class="citation" data-cites="NopW1Vw3">[<a href="#ref-NopW1Vw3">9</a>]</span> to optimize a series of feature selectors, preprocessors and machine learning models with the objective of maximizing classification accuracy. While most AutoML systems primarily focus on model selection and hyperparameter optimization, TPOT also pays attention to feature selection and feature engineering in building a complete pipeline. Applying GP with the NSGA-II Pareto optimization <span class="citation" data-cites="iBP5Naag">[<a href="#ref-iBP5Naag">10</a>]</span>, TPOT optimizes the accuracy achieved by the pipeline while accounting for its complexity. Specifically, to automatically generate and optimize these machine learning pipelines, TPOT utilizes the Python package DEAP <span class="citation" data-cites="Gcs0HrMy">[<a href="#ref-Gcs0HrMy">11</a>]</span> to implement the GP algorithm.</p>
<p>Given no a priori knowledge about the problem, TPOT has been showed to frequently outperform standard machine learning analyses <span class="citation" data-cites="QkGSlAB3 JEn7WIoN">[<a href="#ref-QkGSlAB3">12</a>,<a href="#ref-JEn7WIoN">13</a>]</span>. Effort has been made to specialize TPOT for human genetics research, which results in a useful extended version of TPOT, TPOT-MDR, that features Multifactor Dimensionality Reduction and an Expert Knowledge Filter <span class="citation" data-cites="AvvI4W9K">[<a href="#ref-AvvI4W9K">14</a>]</span>. However, at the current stage, TPOT still requires great computational expense to analyze large datasets such as in genome-wide association studies or gene expression analyses. Consequently, application of TPOT on real-world datasets has been limited to small sets of features <span class="citation" data-cites="3LGbkjqK">[<a href="#ref-3LGbkjqK">15</a>]</span>.</p>
<p>In this work, we introduce two new features implemented in TPOT that helps increase the system’s scalability. First, the Dataset Selector (DS) allows the users to pass specific subsets of the features, reducing the computational expense of TPOT at the beginning of each pipeline to only evaluate on a smaller subset of data rather than the entire dataset. Consequently, DS makes TPOT applicable on large data sets by slicing the data into smaller sets of features (<em>e.g.</em> genes) and allowing genetic algorithm to select the best subset in the final pipeline. Second, Template enables the option for strongly typed GP, a method to enforce type constraints in genetic programming. By letting users specify a desired structure of the resulting machine learning pipeline, Template helps reduce TPOT computation time and potentially provide more interpretable results.</p>
<h2 id="methods">Methods</h2>
<p>We begin with description of the two novel additiona to TPOT, Dataset Selector and Template. Then, we provide detail of a real-world RNA-Seq expression dataset and describe a simulation approach to generate data comparable with the expression data. Finally, we discuss other methods and performance metrics for comparison. The <code>R</code> and <code>Python</code> scripts for simulation and analysis are publicly available on the GitHub repository https://github.com/lelaboratoire/tpot-ds.</p>
<h3 id="dataset-selector">Dataset Selector</h3>
<p>TPOT’s current operators include sets of feature pre-processors, feature transformers, feature selection techniques, and supervised classifiers and regressions. In this study, we introduce a new operator called Dataset Selector (DS) that enables biologically guided group-level feature selection. Specifically, taking place at the very first stage of the pipeline, DS passes only a specific subset of the features onwards. Hence, with DS, users can specify subsets of features of interest to reduce the feature space’s dimension at pipeline initialization. From predefined subsets of features, the DS operator allows TPOT to select the best subset that maximize average accuracy in k-fold cross validation (5-fold by default).</p>
<p>For example, in a gene expression analysis of major depressive disorder, a neuroscientist can specify collections of genes in pathways of interest and identify the important collection that helps predict the depression severity. Similarly, in a genome-wide association study of breast cancer, an analyst may assign variants in the data to different subsets of potentially related variants and detect the subset associated with the breast cancer diagnosis. In general, the DS operator allows for compartmentalization the feature space to smaller subsets based on <em>a priori</em> expert knowledge about the biomedical dataset. From here, TPOT selects the most relevant group of features, which can be utilized to motivate further analysis on that small group of features in biomedical research.</p>
<h3 id="template">Template</h3>
<p>Parallel with the establishment of the Dataset Selector operator, we now offer TPOT users the option to define a Template that provides a way to specify a desired structure for the resulting machine learning pipeline, which will reduce TPOT computation time and potentially provide more interpretable results.</p>
<p>Current implementation of Template supports linear pipelines, or path graphs, which are trees with two nodes (operators) of vertex degree 1, and the other <span class="math inline">\(n-2\)</span> nodes of vertex degree 2. Further, Template takes advantage of the strongly typed genetic programming framework that enforces data-type constraints <span class="citation" data-cites="KgFuJ0Jv">[<a href="#ref-KgFuJ0Jv">16</a>]</span> and imposes type-based restrictions on which element (<em>i.e.</em>, operator) type can be chosen at each node. In strongly typed genetic programming, while the fitness function and parameters remain the same, the initialization procedure and genetic operators (<em>e.g.</em>, mutation, crossover) must respect the enhanced legality constraints <span class="citation" data-cites="KgFuJ0Jv">[<a href="#ref-KgFuJ0Jv">16</a>]</span>. With a Template defined, each node in the tree pipeline is assigned one of the five major operator types: dataset selector, feature selection, feature transform, classifier or regressor. Moreover, besides the major operator types, each node can also be assigned more specifically as a method of an operator, such as decision trees for classifier. An example Template is Dataset selector → Feature transform → Decision trees.</p>
<h3 id="datasets">Datasets</h3>
<p>We apply TPOT with the new DS operator on both simulated datasets and a real world RNA-Seq gene expression dataset. With both real-world and simulated data, we hope to acquire a comprehensive view of the strengths and limitations of TPOT in the next generation sequencing domain.</p>
<h4 id="real-world-rna-seq-expression-data">Real-world RNA-Seq expression data</h4>
<p>We employed TPOT-DS on an RNA-Seq expression dataset of 78 individuals with major depressive disorder (MDD) and 79 healthy controls (HC) from Ref. <span class="citation" data-cites="p7dAO241">[<a href="#ref-p7dAO241">17</a>]</span>. Gene expression levels were quantified from reads of 19,968 annotated protein-coding genes and underwent a series of preprocessing steps including low read-count and outlier removal, technical and batch effect adjustment, and coefficient of variation filtering. Consequently, whole blood RNA-Seq measurements of 5,912 genes were obtained and are now used in the current study to test for association with MDD status. We use the 23 subsets of interconnected genes called depression gene modules (DGMs) identified from the RNA-Seq gene network module analysis <span class="citation" data-cites="p7dAO241">[<a href="#ref-p7dAO241">17</a>]</span> as input for the DS operator.</p>
<h4 id="simulation-methods">Simulation methods</h4>
<p>The simulated datasets were generated using the <code>R</code> package <code>privateEC</code>, which was designed to simulate realistic effects to be expected in gene expression or resting-state fMRI data. In the current study, to be consistent with the real expression dataset, we simulate interaction effect data with <em>m</em> = 200 individuals (100 cases and 100 controls) and <em>p</em>= 5,000 real-valued features with 4% functional (true positive association with outcome) for each training and testing set. Full details of the simulation approach can be found in Refs. <span class="citation" data-cites="p7dAO241 NKnMeQUs">[<a href="#ref-p7dAO241">17</a>,<a href="#ref-NKnMeQUs">18</a>]</span>. Briefly, the privateEC simulation induces a differential co-expression network random normal expression levels and permute the values of targeted features within the cases to generate interactions. Further, by imposing a large number of background features (no association with outcome), we seek to assess TPOT-DS’s performance in accommodating large numbers of non-predictive features.</p>
<p>To closely resemble the module size distribution in the RNA-Seq data, we first fit a <span class="math inline">\(\Gamma\)</span> distribution to the observed module sizes then sample from this distribution values for the simulated subset size, before the total number of features reaches 4,800 (number of background features). Then, the background features were randomly placed in each subset corresponding to its size. Also, for each subset <span class="math inline">\(S_i, i = 1, \dots, n\)</span>, a functional feature <span class="math inline">\(s_j\)</span> belongs to the subset with the probability <span id="eq:p_subset" style="display: inline-block; position: relative; width: 100%"><span class="math display">\[P(s_j \in S_i) \sim 1.618^{-i}\]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(1)</span></span>  where 1.618 is an approximation of the golden ratio and yields a reasonable distribution of the functional features: they are more likely to be included in the earlier subsets (subset 1 and 2) than the later ones.</p>
<h3 id="performance-assessment">Performance assessment</h3>
<p>For each simulated and real-world dataset, after randomly splitting the entire data in two balanced smaller sets (75% training and 25% holdout), we trained TPOT-DS with the Template <code>Dataset Selector-Transformer-Classifier</code> on training data to predict class (e.g., diagnostic phenotype in real-world data) in the holdout set. We assess the performance of TPOT-DS by quantifying its ability to correctly select the most important subset (containing most functional features) in 100 replicates of TPOT runs on simulated data with known underlying truth. We also compare the out-of-sample accuracy of TPOT-DS’s exported pipeline on the holdout set with that of standard TPOT (with <code>Transformer-Classifier</code> Template, no DS operator) and XGBoost <span class="citation" data-cites="8w9fI63O">[<a href="#ref-8w9fI63O">19</a>]</span>, a fast and an efficient implementation of the gradient tree boosting method that has shown much utility in many winning Kaggle solutions <span class="citation" data-cites="1MHQyfXY">[<a href="#ref-1MHQyfXY">20</a>]</span> and been successfully incorporated in several neural network architectures <span class="citation" data-cites="19eUrsX1M 13as7dipI">[<a href="#ref-19eUrsX1M">21</a>,<a href="#ref-13as7dipI">22</a>]</span>. In the family of gradient boosted decision trees, XGBoost accounts for complex non-linear interaction structure among features and leverages gradient descents and boosting (sequential ensemble of weak classifiers) to effectively produce a strong prediction model. To obtain the optimal performance for this baseline model, we tune XGBoost hyperparameters using the <code>R</code> package <code>caret</code> <span class="citation" data-cites="6MvKCe21">[<a href="#ref-6MvKCe21">23</a>]</span> version 6.0-80 with the repeated cross-validation algorithm and random search method.</p>
<h2 id="results">Results</h2>
<p>Our main goal is to test the performance of methods to identify features that discriminate between groups and optimize the classification accuracy.</p>
<h3 id="simulated-data">Simulated data</h3>
<p>We compare the accuracy of each method for <em>r</em> = 100 replicate simulated data sets with moderate interaction effect. These values of the effect size in the simulations generate adequately challenging data sets so that the methods’ accuracies stay moderate and do not cluster around 0.5 or 1. Each replicate data set is split into training and holdout. The TPOT-DS, standard TPOT and XGBoost models are built from the training dataset, then the trained model is applied to the independent holdout data to obtain the generalization accuracy.</p>
<p>Our simulation design produces a reasonable distribution of the functional features in all subsets, of which proportions are shown in Table [S1]. According to Eq. <a href="#eq:p_subset">1</a>, the earlier the subset, the more functional features it has. Therefore, our first aim is to determine how well TPOT-DS can identify the first subset 1 that contains the largest number of informative features. With the specified template <code>Dataset Selector-Transformer-Classifier</code>, in 100 replications, TPOT-DS correctly selects subset 1 in the resulting pipeline 75 times (Fig. <a href="#fig:simDS">1</a>), with an average cross-validated accuracy on the training set of 0.73 and out-of-sample accuracy of 0.69.</p>
<figure>
<img src="images/sim_100.svg" alt="Figure 1: TPOT-DS’s out-of-sample accuracy in simulated data with selected subset" id="fig:simDS" style="width:7in;height:4in" /><figcaption><span>Figure 1:</span> TPOT-DS’s out-of-sample accuracy in simulated data with selected subset</figcaption>
</figure>
<p>Without DS, the standard TPOT and tuned XGBoost models respectively report a cross-validated accuracy of [0.661] and 0.533, and out-of-sample accuracy of [0.565] and 0.575.</p>
<h3 id="rna-seq-expression-data">RNA-Seq expression data</h3>
<p>We apply standard TPOT, TPOT-DS and XGBoost to the RNA-Seq study of 78 major depressive disorder (MDD) subjects and 79 healthy controls (HC) described in <span class="citation" data-cites="p7dAO241">[<a href="#ref-p7dAO241">17</a>]</span>. The dataset contains 5,912 genes after preprocessing and filtering (see Methods for more detail). We excluded 277 genes that did not belong to 23 subsets of interconnected genes (DGMs) so that the dataset remains the same across the three methods. As with simulated data, all models are built from the training dataset (61 HC, 56 MDD), then the trained model is applied to the independent holdout data (18 HC, 22 MDD) to obtain the generalization accuracy.</p>
<p>In 100 replications, TPOT-DS selects DGM-5 (291 genes) 64 times to be the subset most predictive of the diagnosis status (Fig. <a href="#fig:realDS">2</a>), with an average cross-validated accuracy on the training set of 0.715 and out-of-sample accuracy of 0.636. In the previous study with a modular network approach, we showed that DGM-5 has statistically significant associations with depression severity measured by the Montgomery-Åsberg Depression Scale (MADRS). Further, with 82% overlap of DGM-5’s genes in a separate dataset from the RNA-Seq study by Mostafavi et al. <span class="citation" data-cites="g454CrrS">[<a href="#ref-g454CrrS">24</a>]</span>, this gene collection’s enrichment score was also shown to be significantly associated with the diagnosis status in this independent dataset.</p>
<figure>
<img src="images/real_100.svg" alt="Figure 2: TPOT-DS’s out-of-sample accuracy in RNA-Seq expression data with selected subset" id="fig:realDS" style="width:5in;height:4in" /><figcaption><span>Figure 2:</span> TPOT-DS’s out-of-sample accuracy in RNA-Seq expression data with selected subset</figcaption>
</figure>
<p>After DGM-5, DGM-13 (134 genes) was selected by TPOT-DS 30 times (Fig. <a href="#fig:realDS">2</a>), with an average cross-validated accuracy on the training set of 0.717 and out-of-sample accuracy of 0.563. Previously, this module’s enrichment score did not show statistically significant association with the MADRS.</p>
<p>Without DS, the standard TPOT and tuned XGBoost models respectively report a cross-validated accuracy of [] and 0.543, and out-of-sample accuracy of [] and 0.525.</p>
<h2 id="discussion">Discussion</h2>
<p>To our knowledge, TPOT-DS is the first AutoML tool to offer the option of feature selection at the group level. Previously, it was computationally expensive for any AutoML program to process biomedical big data. TPOT-DS is able to identify the most meaningful group of features to include in the prediction pipeline. We assessed TPOT-DS’s out-of-sample prediction accuracy compared to standard TPOT and XGBoost, another state-of-the-art machine learning method. We applied TPOT-DS to real-world expression data to demonstrate the identification of biologically relevant groups of genes.</p>
<p>Implemented with a strongly typed GP, Template allows users to pre-specify a particular pipeline structure, which speeds up the automation computation time and provides potentially more interpretable results. Hence, Template enables the comparison between the two TPOT implementations, with and without DS.</p>
<p>We simulated data of the similar scale and chalenging enough for the models to have similar predictive power as in the real-world RNA-Seq data. TPOT-DS correctly selects the first subset (containing the most important features) 75% of the time with high holdout accuracy (0.69). When another subset is chosen in the final pipeline, this method still produces holdout accuracy comparable to that of standard TPOT and XGBoost (0.565 - 0.575).</p>
<p>Interestingly enough, TPOT-DS repeatedly selects DGM-5 to include in the final pipeline. In a previous study, we showed DGM-5 and DGM-17 enrichment scores were significantly associated with depression severity <span class="citation" data-cites="p7dAO241">[<a href="#ref-p7dAO241">17</a>]</span>. We also remarked that DGM-5 contains many genes that are biologically relevant or previously associated with mood disorders <span class="citation" data-cites="p7dAO241">[<a href="#ref-p7dAO241">17</a>]</span> and its enriched pathways such as apoptosis indicates a genetic signature of MDD pertaining shrinkage of brain region-specific volume due to cell loss <span class="citation" data-cites="19yG9lS3X Okd6uiRx">[<a href="#ref-19yG9lS3X">25</a>,<a href="#ref-Okd6uiRx">26</a>]</span>.</p>
<p>TPOT-DS also select DGM-13 as a potentially predictive group of features with smaller out-of-sample accuracy compared to DGM-5 (0.563 <span class="math inline">\(&lt;\)</span> 0.636). []</p>
<p>It is important to discuss the complexity - interpretability trade-off in the context of AutoML. While arbitrarily-shaped pipelines may yield predictions competitive to human-level performance, these pipelines are often too complex to be interpretable. Vice versa, a simpler pipeline with defined steps of operators may be easier to interpret but not yield the optimal accuracy. Finding the optimal pipeline complexity that yields reasonable model interpretation and generalization remains a challenging task for AutoML application in biomedical big data.</p>
<p>Another limitation of this analysis is that subsets have to be predefined prior to executing TPOT-DS. While this option is desirable when <em>a prior</em> knowledge on the biological data is available, it might pose as a challenge when this is not the case, such as when analyzing data of a brand-new disease. Nevertheless, one can perform a clustering method such as <em>k</em>-means to group features prior to performing TPOT-DS on the data.</p>
<p>Extensions of TPOT-DS will involve overlapping subsets, which will require pipeline complexity reformulation beyond the total number of operators included in a pipeline. Also, a future design to support tree structures for Template will enable TPOT-DS to identify more than one subset that have high predictive power of the outcome.</p>
<h2 id="references" class="page_break_before">References</h2>
<!-- Explicitly insert bibliography here -->
<div id="refs">
<div id="ref-8JQDv397">
<p>1. <strong>Efficient and Robust Automated Machine Learning</strong><br />
Matthias Feurer, Aaron Klein, Katharina Eggensperger, Jost Springenberg, Manuel Blum, Frank Hutter<br />
(2015) <a href="https://papers.nips.cc/paper/5872-efficient-and-robust-automated-machine-learning">https://papers.nips.cc/paper/5872-efficient-and-robust-automated-machine-learning</a></p>
</div>
<div id="ref-S6aZVb3n">
<p>2. <strong>Auto-WEKA: Combined Selection and Hyperparameter Optimization of Classification Algorithms</strong><br />
Chris Thornton, Frank Hutter, Holger H. Hoos, Kevin Leyton-Brown<br />
<em>arXiv</em> (2012-08-18) <a href="https://arxiv.org/abs/1208.3719v2">https://arxiv.org/abs/1208.3719v2</a></p>
</div>
<div id="ref-ai67wdhp">
<p>3. <strong>Auto-WEKA 2.0: Automatic model selection and hyperparameter optimization in WEKA</strong><br />
Lars Kotthoff, Chris Thornton, Holger H. Hoos, Frank Hutter, Kevin Leyton-Brown<br />
<em>Journal of Machine Learning Research</em> (2017) <a href="http://www.jmlr.org/papers/v18/16-261.html">http://www.jmlr.org/papers/v18/16-261.html</a></p>
</div>
<div id="ref-6ChydIkb">
<p>4. <strong>RECIPE: A Grammar-Based Framework for Automatically Evolving Classification Pipelines</strong><br />
Alex G. C. de Sá, Walter José G. S. Pinto, Luiz Otavio V. B. Oliveira, Gisele L. Pappa<br />
<em>Lecture Notes in Computer Science</em> (2017) <a href="https://doi.org/gfjzg2">https://doi.org/gfjzg2</a><br />
DOI: <a href="https://doi.org/10.1007/978-3-319-55696-3_16">10.1007/978-3-319-55696-3_16</a></p>
</div>
<div id="ref-RiocGZOq">
<p>5. <strong>Autostacker: A Compositional Evolutionary Learning System</strong><br />
Boyuan Chen, Harvey Wu, Warren Mo, Ishanu Chattopadhyay, Hod Lipson<br />
<em>arXiv</em> (2018-03-02) <a href="https://arxiv.org/abs/1803.00684v1">https://arxiv.org/abs/1803.00684v1</a></p>
</div>
<div id="ref-1CQSBxFFr">
<p>6. <strong>joeddav/devol</strong><br />
joeddav<br />
<em>GitHub</em> <a href="https://github.com/joeddav/devol">https://github.com/joeddav/devol</a></p>
</div>
<div id="ref-sOdEzGT3">
<p>7. <strong>AutoML: Automatic Machine Learning — H2O 3.22.0.1 documentation</strong>(2018-10-26) <a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html">http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html</a></p>
</div>
<div id="ref-HA8l3lpi">
<p>8. <strong>reiinakano/xcessiv</strong><br />
reiinakano<br />
<em>GitHub</em> <a href="https://github.com/reiinakano/xcessiv">https://github.com/reiinakano/xcessiv</a></p>
</div>
<div id="ref-NopW1Vw3">
<p>9. <strong>Genetic Programming: An Introduction: on the Automatic Evolution of Computer Programs and Its Applications</strong><br />
Wolfgang Banzhaf, Frank D. Francone, Robert E. Keller, Nordin Peter<br />
<em>ACM Digital Library</em> (1998) <a href="https://dl.acm.org/citation.cfm?id=280485">https://dl.acm.org/citation.cfm?id=280485</a></p>
</div>
<div id="ref-iBP5Naag">
<p>10. <strong>A fast and elitist multiobjective genetic algorithm: NSGA-II</strong><br />
K. Deb, A. Pratap, S. Agarwal, T. Meyarivan<br />
<em>IEEE Transactions on Evolutionary Computation</em> (2002-04) <a href="https://doi.org/bnw2vv">https://doi.org/bnw2vv</a><br />
DOI: <a href="https://doi.org/10.1109/4235.996017">10.1109/4235.996017</a></p>
</div>
<div id="ref-Gcs0HrMy">
<p>11. <strong>DEAP: Evolutionary Algorithms Made Easy</strong><br />
Félix-Antoine Fortin, François-Michel De Rainville, Marc-André Gardner, Marc Parizeau, Christian Gagné<br />
<em>Journal of Machine Learning Research</em> (2012) <a href="http://www.jmlr.org/papers/v13/fortin12a.html">http://www.jmlr.org/papers/v13/fortin12a.html</a></p>
</div>
<div id="ref-QkGSlAB3">
<p>12. <strong>Evaluation of a Tree-based Pipeline Optimization Tool for Automating Data Science</strong><br />
Randal S. Olson, Nathan Bartley, Ryan J. Urbanowicz, Jason H. Moore<br />
<em>Proceedings of the 2016 on Genetic and Evolutionary Computation Conference - GECCO ’16</em> (2016) <a href="https://doi.org/gfgqv2">https://doi.org/gfgqv2</a><br />
DOI: <a href="https://doi.org/10.1145/2908812.2908918">10.1145/2908812.2908918</a></p>
</div>
<div id="ref-JEn7WIoN">
<p>13. <strong>Identifying and Harnessing the Building Blocks of Machine Learning Pipelines for Sensible Initialization of a Data Science Automation Tool</strong><br />
Randal S. Olson, Jason H. Moore<br />
<em>arXiv</em> (2016-07-29) <a href="https://arxiv.org/abs/1607.08878v1">https://arxiv.org/abs/1607.08878v1</a></p>
</div>
<div id="ref-AvvI4W9K">
<p>14. <strong>Toward the automated analysis of complex diseases in genome-wide association studies using genetic programming</strong><br />
Andrew Sohn, Randal S. Olson, Jason H. Moore<br />
<em>Proceedings of the Genetic and Evolutionary Computation Conference on - GECCO ’17</em> (2017) <a href="https://doi.org/gfgqv3">https://doi.org/gfgqv3</a><br />
DOI: <a href="https://doi.org/10.1145/3071178.3071212">10.1145/3071178.3071212</a></p>
</div>
<div id="ref-3LGbkjqK">
<p>15. <strong>Integrated machine learning pipeline for aberrant biomarker enrichment (i-mAB): characterizing clusters of differentiation within a compendium of systemic lupus erythematosus patients</strong><br />
Trang T. Le, Nigel O. Blackwood, Jaclyn N. Taroni, Weixuan Fu, Matthew K. Breitenstein<br />
<em>arXiv</em> (2018-03-08) <a href="https://arxiv.org/abs/1803.04487v1">https://arxiv.org/abs/1803.04487v1</a></p>
</div>
<div id="ref-KgFuJ0Jv">
<p>16. <strong>Strongly Typed Genetic Programming</strong><br />
David J. Montana<br />
<em>Evolutionary Computation</em> (1995-06) <a href="https://doi.org/ct3mnb">https://doi.org/ct3mnb</a><br />
DOI: <a href="https://doi.org/10.1162/evco.1995.3.2.199">10.1162/evco.1995.3.2.199</a></p>
</div>
<div id="ref-p7dAO241">
<p>17. <strong>Identification and replication of RNA-Seq gene network modules associated with depression severity</strong><br />
Trang T. Le, Jonathan Savitz, Hideo Suzuki, Masaya Misaki, T. Kent Teague, Bill C. White, Julie H. Marino, Graham Wiley, Patrick M. Gaffney, Wayne C. Drevets, … Jerzy Bodurka<br />
<em>Translational Psychiatry</em> (2018-09-05) <a href="https://doi.org/gd7jx7">https://doi.org/gd7jx7</a><br />
DOI: <a href="https://doi.org/10.1038/s41398-018-0234-3">10.1038/s41398-018-0234-3</a> · PMID: <a href="http://www.ncbi.nlm.nih.gov/pubmed/30185774">30185774</a> · PMCID: <a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6125582">PMC6125582</a></p>
</div>
<div id="ref-NKnMeQUs">
<p>18. <strong>Differential co-expression network centrality and machine learning feature selection for identifying susceptibility hubs in networks with scale-free structure</strong><br />
Caleb A Lareau, Bill C White, Ann L Oberg, Brett A McKinney<br />
<em>BioData Mining</em> (2015-02-03) <a href="https://doi.org/gb5fpr">https://doi.org/gb5fpr</a><br />
DOI: <a href="https://doi.org/10.1186/s13040-015-0040-x">10.1186/s13040-015-0040-x</a> · PMID: <a href="http://www.ncbi.nlm.nih.gov/pubmed/25685197">25685197</a> · PMCID: <a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4326454">PMC4326454</a></p>
</div>
<div id="ref-8w9fI63O">
<p>19. <strong>XGBoost</strong><br />
Tianqi Chen, Carlos Guestrin<br />
<em>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD ’16</em> (2016) <a href="https://doi.org/gdp84q">https://doi.org/gdp84q</a><br />
DOI: <a href="https://doi.org/10.1145/2939672.2939785">10.1145/2939672.2939785</a></p>
</div>
<div id="ref-1MHQyfXY">
<p>20. <strong>Kaggle: Your Home for Data Science</strong><a href="https://www.kaggle.com/">https://www.kaggle.com/</a></p>
</div>
<div id="ref-19eUrsX1M">
<p>21. <strong>Short-Term Load Forecasting Using EMD-LSTM Neural Networks with a Xgboost Algorithm for Feature Importance Evaluation</strong><br />
Huiting Zheng, Jiabin Yuan, Long Chen<br />
<em>Energies</em> (2017-08-08) <a href="https://doi.org/gbtqbr">https://doi.org/gbtqbr</a><br />
DOI: <a href="https://doi.org/10.3390/en10081168">10.3390/en10081168</a></p>
</div>
<div id="ref-13as7dipI">
<p>22. <strong>A Novel Image Classification Method with CNN-XGBoost Model</strong><br />
Xudie Ren, Haonan Guo, Shenghong Li, Shilin Wang, Jianhua Li<br />
<em>Digital Forensics and Watermarking</em> (2017) <a href="https://doi.org/gfgvf3">https://doi.org/gfgvf3</a><br />
DOI: <a href="https://doi.org/10.1007/978-3-319-64185-0_28">10.1007/978-3-319-64185-0_28</a></p>
</div>
<div id="ref-6MvKCe21">
<p>23. <strong>Building Predictive Models inRUsing thecaretPackage</strong><br />
Max Kuhn<br />
<em>Journal of Statistical Software</em> (2008) <a href="https://doi.org/gdgzwf">https://doi.org/gdgzwf</a><br />
DOI: <a href="https://doi.org/10.18637/jss.v028.i05">10.18637/jss.v028.i05</a></p>
</div>
<div id="ref-g454CrrS">
<p>24. <strong>Type I interferon signaling genes in recurrent major depression: increased expression detected by whole-blood RNA sequencing</strong><br />
S Mostafavi, A Battle, X Zhu, JB Potash, MM Weissman, J Shi, K Beckman, C Haudenschild, C McCormick, R Mei, … DF Levinson<br />
<em>Molecular Psychiatry</em> (2013-12-03) <a href="https://doi.org/f6qdpr">https://doi.org/f6qdpr</a><br />
DOI: <a href="https://doi.org/10.1038/mp.2013.161">10.1038/mp.2013.161</a> · PMID: <a href="http://www.ncbi.nlm.nih.gov/pubmed/24296977">24296977</a> · PMCID: <a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5404932">PMC5404932</a></p>
</div>
<div id="ref-19yG9lS3X">
<p>25. <strong>A meta-analysis examining clinical predictors of hippocampal volume in patients with major depressive disorder.</strong><br />
Margaret C McKinnon, Kaan Yucel, Anthony Nazarov, Glenda M MacQueen<br />
<em>Journal of psychiatry &amp; neuroscience : JPN</em> (2009-01) <a href="https://www.ncbi.nlm.nih.gov/pubmed/19125212">https://www.ncbi.nlm.nih.gov/pubmed/19125212</a><br />
PMID: <a href="http://www.ncbi.nlm.nih.gov/pubmed/19125212">19125212</a> · PMCID: <a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2612082">PMC2612082</a></p>
</div>
<div id="ref-Okd6uiRx">
<p>26. <strong>Increased apoptosis in patients with major depression: A preliminary study.</strong><br />
E Eilat, S Mendlovic, A Doron, V Zakuth, Z Spirer<br />
<em>Journal of immunology (Baltimore, Md. : 1950)</em> (1999-07-01) <a href="https://www.ncbi.nlm.nih.gov/pubmed/10384158">https://www.ncbi.nlm.nih.gov/pubmed/10384158</a><br />
PMID: <a href="http://www.ncbi.nlm.nih.gov/pubmed/10384158">10384158</a></p>
</div>
</div>
<script>
// AnchorJS minified version below.
// Source https://github.com/bryanbraun/anchorjs/blob/064abdd0987f305933ec4982af6d0c1cf2fd0814/anchor.js

/**
 * AnchorJS - v4.0.0 - 2017-06-02
 * https://github.com/bryanbraun/anchorjs
 * Copyright (c) 2017 Bryan Braun; Licensed MIT
 */
!function(A,e){"use strict";"function"==typeof define&&define.amd?define([],e):"object"==typeof module&&module.exports?module.exports=e():(A.AnchorJS=e(),A.anchors=new A.AnchorJS)}(this,function(){"use strict";function A(A){function e(A){A.icon=A.hasOwnProperty("icon")?A.icon:"",A.visible=A.hasOwnProperty("visible")?A.visible:"hover",A.placement=A.hasOwnProperty("placement")?A.placement:"right",A.class=A.hasOwnProperty("class")?A.class:"",A.truncate=A.hasOwnProperty("truncate")?Math.floor(A.truncate):64}function t(A){var e;if("string"==typeof A||A instanceof String)e=[].slice.call(document.querySelectorAll(A));else{if(!(Array.isArray(A)||A instanceof NodeList))throw new Error("The selector provided to AnchorJS was invalid.");e=[].slice.call(A)}return e}function n(){if(null===document.head.querySelector("style.anchorjs")){var A,e=document.createElement("style");e.className="anchorjs",e.appendChild(document.createTextNode("")),void 0===(A=document.head.querySelector('[rel="stylesheet"], style'))?document.head.appendChild(e):document.head.insertBefore(e,A),e.sheet.insertRule(" .anchorjs-link {   opacity: 0;   text-decoration: none;   -webkit-font-smoothing: antialiased;   -moz-osx-font-smoothing: grayscale; }",e.sheet.cssRules.length),e.sheet.insertRule(" *:hover > .anchorjs-link, .anchorjs-link:focus  {   opacity: 1; }",e.sheet.cssRules.length),e.sheet.insertRule(" [data-anchorjs-icon]::after {   content: attr(data-anchorjs-icon); }",e.sheet.cssRules.length),e.sheet.insertRule(' @font-face {   font-family: "anchorjs-icons";   src: url(data:n/a;base64,AAEAAAALAIAAAwAwT1MvMg8yG2cAAAE4AAAAYGNtYXDp3gC3AAABpAAAAExnYXNwAAAAEAAAA9wAAAAIZ2x5ZlQCcfwAAAH4AAABCGhlYWQHFvHyAAAAvAAAADZoaGVhBnACFwAAAPQAAAAkaG10eASAADEAAAGYAAAADGxvY2EACACEAAAB8AAAAAhtYXhwAAYAVwAAARgAAAAgbmFtZQGOH9cAAAMAAAAAunBvc3QAAwAAAAADvAAAACAAAQAAAAEAAHzE2p9fDzz1AAkEAAAAAADRecUWAAAAANQA6R8AAAAAAoACwAAAAAgAAgAAAAAAAAABAAADwP/AAAACgAAA/9MCrQABAAAAAAAAAAAAAAAAAAAAAwABAAAAAwBVAAIAAAAAAAIAAAAAAAAAAAAAAAAAAAAAAAMCQAGQAAUAAAKZAswAAACPApkCzAAAAesAMwEJAAAAAAAAAAAAAAAAAAAAARAAAAAAAAAAAAAAAAAAAAAAQAAg//0DwP/AAEADwABAAAAAAQAAAAAAAAAAAAAAIAAAAAAAAAIAAAACgAAxAAAAAwAAAAMAAAAcAAEAAwAAABwAAwABAAAAHAAEADAAAAAIAAgAAgAAACDpy//9//8AAAAg6cv//f///+EWNwADAAEAAAAAAAAAAAAAAAAACACEAAEAAAAAAAAAAAAAAAAxAAACAAQARAKAAsAAKwBUAAABIiYnJjQ3NzY2MzIWFxYUBwcGIicmNDc3NjQnJiYjIgYHBwYUFxYUBwYGIwciJicmNDc3NjIXFhQHBwYUFxYWMzI2Nzc2NCcmNDc2MhcWFAcHBgYjARQGDAUtLXoWOR8fORYtLTgKGwoKCjgaGg0gEhIgDXoaGgkJBQwHdR85Fi0tOAobCgoKOBoaDSASEiANehoaCQkKGwotLXoWOR8BMwUFLYEuehYXFxYugC44CQkKGwo4GkoaDQ0NDXoaShoKGwoFBe8XFi6ALjgJCQobCjgaShoNDQ0NehpKGgobCgoKLYEuehYXAAAADACWAAEAAAAAAAEACAAAAAEAAAAAAAIAAwAIAAEAAAAAAAMACAAAAAEAAAAAAAQACAAAAAEAAAAAAAUAAQALAAEAAAAAAAYACAAAAAMAAQQJAAEAEAAMAAMAAQQJAAIABgAcAAMAAQQJAAMAEAAMAAMAAQQJAAQAEAAMAAMAAQQJAAUAAgAiAAMAAQQJAAYAEAAMYW5jaG9yanM0MDBAAGEAbgBjAGgAbwByAGoAcwA0ADAAMABAAAAAAwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABAAH//wAP) format("truetype"); }',e.sheet.cssRules.length)}}this.options=A||{},this.elements=[],e(this.options),this.isTouchDevice=function(){return!!("ontouchstart"in window||window.DocumentTouch&&document instanceof DocumentTouch)},this.add=function(A){var i,o,s,c,r,a,h,l,u,d,f,g,p=[];if(e(this.options),"touch"===(g=this.options.visible)&&(g=this.isTouchDevice()?"always":"hover"),A||(A="h2, h3, h4, h5, h6"),0===(i=t(A)).length)return this;for(n(),o=document.querySelectorAll("[id]"),s=[].map.call(o,function(A){return A.id}),r=0;r<i.length;r++)if(this.hasAnchorJSLink(i[r]))p.push(r);else{if(i[r].hasAttribute("id"))c=i[r].getAttribute("id");else if(i[r].hasAttribute("data-anchor-id"))c=i[r].getAttribute("data-anchor-id");else{u=l=this.urlify(i[r].textContent),h=0;do{void 0!==a&&(u=l+"-"+h),a=s.indexOf(u),h+=1}while(-1!==a);a=void 0,s.push(u),i[r].setAttribute("id",u),c=u}d=c.replace(/-/g," "),(f=document.createElement("a")).className="anchorjs-link "+this.options.class,f.href="#"+c,f.setAttribute("aria-label","Anchor link for: "+d),f.setAttribute("data-anchorjs-icon",this.options.icon),"always"===g&&(f.style.opacity="1"),""===this.options.icon&&(f.style.font="1em/1 anchorjs-icons","left"===this.options.placement&&(f.style.lineHeight="inherit")),"left"===this.options.placement?(f.style.position="absolute",f.style.marginLeft="-1em",f.style.paddingRight="0.5em",i[r].insertBefore(f,i[r].firstChild)):(f.style.paddingLeft="0.375em",i[r].appendChild(f))}for(r=0;r<p.length;r++)i.splice(p[r]-r,1);return this.elements=this.elements.concat(i),this},this.remove=function(A){for(var e,n,i=t(A),o=0;o<i.length;o++)(n=i[o].querySelector(".anchorjs-link"))&&(-1!==(e=this.elements.indexOf(i[o]))&&this.elements.splice(e,1),i[o].removeChild(n));return this},this.removeAll=function(){this.remove(this.elements)},this.urlify=function(A){var t=/[& +$,:;=?@"#{}|^~[`%!'<>\]\.\/\(\)\*\\]/g;return this.options.truncate||e(this.options),A.trim().replace(/\'/gi,"").replace(t,"-").replace(/-{2,}/g,"-").substring(0,this.options.truncate).replace(/^-+|-+$/gm,"").toLowerCase()},this.hasAnchorJSLink=function(A){var e=A.firstChild&&(" "+A.firstChild.className+" ").indexOf(" anchorjs-link ")>-1,t=A.lastChild&&(" "+A.lastChild.className+" ").indexOf(" anchorjs-link ")>-1;return e||t||!1}}return A});

// Enable links for selected headers
var anchors = new AnchorJS();
anchors.add("h2, h3, h4")
</script>
<!-- Enable Hypothesis annotation. https://web.hypothes.is/ -->
<script src="https://hypothes.is/embed.js" async></script>
</body>
</html>
